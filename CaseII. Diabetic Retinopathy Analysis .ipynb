{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case II: Diabetic Retinopathy Analysis<br>\n",
    "MatiUllah Khan<br>\n",
    "Last edited: 04.03.2018 <br>\n",
    "Cognitive Systems for Health Technology Applications <br>\n",
    "Helsinki Metropolia University of Applied Sciences <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Objectives\n",
    "Our obejectices emphasis on using convolutional neural network to try to diagnose, whether the patient has Diabetic Retinopathy disease. In the training network, pictures of eyes have and donot have disease are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Required libraries and global variables\n",
    "\n",
    "We include the necessary libraries for the pre-processing, and plotting of the given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Required libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report, recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "import keras\n",
    "import keras.layers as layers\n",
    "import keras.models as models\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import VGG16\n",
    "from keras.applications import VGG19\n",
    "% matplotlib inline\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "steps_per_epoch = 32 \n",
    "validation_steps = 16\n",
    "image_height = 200\n",
    "image_width = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data description and preprocessing\n",
    "\n",
    "1. Data augmentation is used to prevent overfitting.\n",
    "2. Validation dataset is not augmentated because you want to accurate validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1237 images belonging to 2 classes.\n",
      "Found 413 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = \"case2_dataset/train\"\n",
    "validation_dir = \"case2_dataset/validation\"\n",
    "test_dir = \"case2_dataset/test\" \n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      zoom_range=0.3,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Generator for train dataset\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size = (image_height, image_width),\n",
    "        batch_size = batch_size,\n",
    "        class_mode = 'binary')\n",
    "\n",
    "# Generator for validation dataset\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size = (image_height, image_width),\n",
    "        batch_size = batch_size,\n",
    "        class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modeling and compilation\n",
    "\n",
    "My plan was to use pretrained convolutional network as base of my model, freeze it and train only self added dense network.\n",
    "\n",
    "I tried VGG16 and VGG19 models with imagenet weights and there where not much difference in the outcome between those two. (VGG16 has 16 layers and VGG19 has 19). I also tried different kind of dense networks but the accuracy was pretty much the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Model)                (None, 6, 4, 512)         20024384  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12288)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1572992   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 21,597,505\n",
      "Trainable params: 1,573,121\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base = VGG19(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(image_height, image_width, 3))\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "conv_base.trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 5. Training \n",
    "\n",
    "When training the model I noticed that my computer doesn't use my GPU. Instead it uses CPU (and it was very time consuming). Im not sure why this is happening at this point but I guess that I migth have installed tensorflow cpu version instead of gpu version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "32/32 [==============================] - 1291s 40s/step - loss: 0.6566 - acc: 0.6658 - val_loss: 0.5909 - val_acc: 0.7033\n",
      "Epoch 2/20\n",
      "32/32 [==============================] - 1432s 45s/step - loss: 0.5832 - acc: 0.7021 - val_loss: 0.5719 - val_acc: 0.6974\n",
      "Epoch 3/20\n",
      " 1/32 [..............................] - ETA: 22:38 - loss: 0.5387 - acc: 0.7812"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "h = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch = steps_per_epoch,\n",
    "      verbose = 1,\n",
    "      epochs = epochs,\n",
    "      validation_data = validation_generator,\n",
    "      validation_steps = validation_steps)\n",
    "t2 = time.time()\n",
    "\n",
    "h.history.update({'time_elapsed': t2 - t1})\n",
    "print('Total elapsed time for training: {:.3f} minutes'.format((t2-t1)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Validation\n",
    "\n",
    "Around 10 epochs my model started to overfit a little bit. Model reached it's best accuracy very quickly. Also it was handy to save the model and the history for a later use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save model and history\n",
    "\n",
    "model.save('case2run3.h5')\n",
    "pickle.dump(h.history, open('history_run3.p', 'wb'))\n",
    "\n",
    "# How to load\n",
    "#h = pickle.load(open('history_run2.p', 'rb'))\n",
    "\n",
    "acc = h.history['acc']\n",
    "val_acc = h.history['val_acc']\n",
    "loss = h.history['loss']\n",
    "val_loss = h.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Results and Discussion\n",
    "\n",
    "By choosing 20 epochs, testing with a a overfitted model can be avoid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size = (image_height,image_width),\n",
    "        batch_size = batch_size,\n",
    "        class_mode = 'binary')\n",
    "\n",
    "r = model.evaluate_generator(test_generator, steps = 32)\n",
    "\n",
    "# Loss and accuracy\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_true = np.zeros(413)\n",
    "y_score = np.zeros(413)\n",
    "sample_count = 413\n",
    "i = 0\n",
    "for inputs_batch, labels_batch in test_generator:\n",
    "        predicts_batch = model.predict(inputs_batch)\n",
    "        L = labels_batch.shape[0]\n",
    "        index = range(i, i + L)\n",
    "        y_true[index] = labels_batch.ravel()\n",
    "        y_score[index] = predicts_batch.ravel()\n",
    "        i = i + L\n",
    "        if i >= sample_count:\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "auc = roc_auc_score(y_true, y_score)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1], '--')\n",
    "plt.grid()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve   AUC = {:.3f}'.format(auc))\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(thresholds, 1-fpr, label = 'specificity')\n",
    "plt.plot(thresholds, tpr, label = 'sensitivity')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Threshold value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Metrics\n",
    "\n",
    "th = 0.3\n",
    "\n",
    "acc = accuracy_score(y_true, y_score > th)\n",
    "prec = precision_score(y_true, y_score > th)\n",
    "f1 = f1_score(y_true, y_score > th)\n",
    "recall = recall_score(y_true, y_score > th)\n",
    "\n",
    "print('Accuracy:  {:.3f}'.format(acc))\n",
    "print('Precision: {:.3f}'.format(prec))\n",
    "print('Recall:    {:.3f}'.format(recall))\n",
    "print('F1:        {:.3f}'.format(f1))\n",
    "print('Classification report')\n",
    "print(classification_report(y_true, y_score > th, labels = [1.0, 0.0], target_names = ['Disease', 'Healthy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_score > th).ravel()\n",
    "\n",
    "print('                      Confusion matrix')\n",
    "print('                       True condition')\n",
    "print('                      Positive Negative      Sum')\n",
    "print('Predicted | Positive  {:8} {:8} {:8}'.format(tp, fp, tp + fp))\n",
    "print('condition | Negative  {:8} {:8} {:8}'.format(fn, tn, fn + tn))\n",
    "print('                 Sum  {:8} {:8} {:8}'.format(tp + fn, fp + tn, tp + fp + fn + tn))\n",
    "print(' ')\n",
    "print('Sensitivity: {:.3f}'.format(tp/(tp+fn)))\n",
    "print('Specificity: {:.3f}'.format(tn/(tn+fp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I reached accuray of 75%. I expected to have better accuracy (at least 80%). Mayby with better image preprocessing, and unfreezing and fine-tuning couple last conv layers of the VGG model it would be possible. If you look at the confusion matrix you can see that model recognizes healty eyes correct the most.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Conclusions\n",
    "\n",
    "\n",
    "I learned that convolutional networks perform well in image categorising problems. Pretrained models are good way of making models more accurate and learn faster. Also you don't have to try to make the model from scratch.\n",
    "\n",
    "I had some major difficulties running the training on my GPU and I need to fix that problem before case 3 (or use cloud services)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
